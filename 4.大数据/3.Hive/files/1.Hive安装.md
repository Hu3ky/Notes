# 1.Hive安装  
## 1.1 Hive安装地址  
1. Hive官网地址  
http://hive.apache.org/  
2. 文档查看地址  
https://cwiki.apache.org/confluence/display/Hive/GettingStarted  
3. 下载地址  
http://archive.apache.org/dist/hive/  
4. github地址  
https://github.com/apache/hive  

## 1.2 Hive安装部署
1. Hive安装及配置  
   - 把apache-hive-3.1.2-bin.tar上传到linux的/opt/software目录下  
   - 解压apache-hive-3.1.2-bin.tar到/opt/module/目录下面  
    ```
    [hadoop@hadoop01 software]$ tar -xvf apache-hive-3.1.2-bin.tar -C /opt/module/
    ```
   - 修改环境变量
    ```
    [hadoop@hadoop01 hadoop-3.3.0]$ sudo vi /etc/profile
    添加如下内容:
    ##Hive_HOME
    export HIVE_HOME=/opt/module/apache-hive-3.1.2-bin
    export PATH=${PATH}:${HIVE_HOME}/bin
    ```
   - 修改/opt/module/apache-hive-3.1.2-bin/conf目录下的hive-env.sh.template名称为hive-env.sh  
    ```
    [hadoop@hadoop01 conf]$ mv hive-env.sh.template hive-env.sh
    ```
   - 配置hive-env.sh文件  
     - 配置HADOOP_HOME路径
        ```
        HADOOP_HOME=/opt/module/hadoop-3.3.0/
        ```
     - 配置HIVE_CONF_DIR路径
        ```
        export HIVE_CONF_DIR=/opt/module/apache-hive-3.1.2-bin/conf
        ```

2. 启动并使用Hive
   - 启动Hive
    ```
    [hadoop@hadoop01 apache-hive-3.1.2-bin]$ hive
    ```
   - 使用Hive
    ```
    hive> show databases;
    hive> use default;
    hive> show tables;
    hive> create table test(id int);
    hive> insert into test values(1);
    hive> select * from test;
    ```
   - Hive默认使用的元数据库为derby，开启Hive之后就会占用元数据库，且不与其他客户端共享数据，所以我们需要将Hive的元数据地址改为MySQL

## 1.3 MySql安装  
1. 检查当前系统是否安装过MySQL  
    ```  
    [hadoop@hadoop01 ~]$ rpm -qa | grep mariadb
    [hadoop@hadoop01 ~]$ sudo rpm -e --nodeps mariadb-libs
    ```  
2. 将MySQL安装包拷贝到/opt/software目录下  
    ```  
    [hadoop@hadoop01 software]$ ls -al
    总用量 1637568
    drwxr-xr-x. 2 hadoop hadoop      4096 1月  25 23:15 .
    drwxr-xr-x. 5 root   root        4096 9月  12 16:52 ..
    -rw-rw-r--. 1 hadoop hadoop  50078112 1月  25 23:15 mysql-community-client-8.0.23-1.el7.x86_64.rpm
    -rw-rw-r--. 1 hadoop hadoop    242596 1月  25 23:19 mysql-community-client-plugins-8.0.23-1.el7.x86_64.rpm
    -rw-rw-r--. 1 hadoop hadoop    635504 1月  25 23:15 mysql-community-common-8.0.23-1.el7.x86_64.rpm
    -rw-rw-r--. 1 hadoop hadoop   4834112 1月  25 23:15 mysql-community-libs-8.0.23-1.el7.x86_64.rpm
    -rw-rw-r--. 1 hadoop hadoop   1273816 1月  25 23:15 mysql-community-libs-compat-8.0.23-1.el7.x86_64.rpm
    -rw-rw-r--. 1 hadoop hadoop 543646172 1月  25 23:15 mysql-community-server-8.0.23-1.el7.x86_64.rpm
    -rw-rw-r--. 1 hadoop hadoop  15831040 1月  25 23:15 mysql-connector-java-8.0.23.tar
    ```  
3. 在安装目录下执行rpm安装  
    ```  
    [hadoop@hadoop01 software]$ sudo rpm -ivh mysql-community-common-8.0.23-1.el7.x86_64.rpm
    [hadoop@hadoop01 software]$ sudo rpm -ivh mysql-community-client-plugins-8.0.23-1.el7.x86_64.rpm
    [hadoop@hadoop01 software]$ sudo rpm -ivh mysql-community-libs-8.0.23-1.el7.x86_64.rpm
    [hadoop@hadoop01 software]$ sudo rpm -ivh mysql-community-libs-compat-8.0.23-1.el7.x86_64.rpm
    [hadoop@hadoop01 software]$ sudo rpm -ivh mysql-community-client-8.0.23-1.el7.x86_64.rpm
    [hadoop@hadoop01 software]$ sudo rpm -ivh mysql-community-server-8.0.23-1.el7.x86_64.rpm
    ```  
4. 删除/etc/my.cnf文件中datadir指向的目录下的所有内容,如果有内容的情况下: 查看datadir的值  
    ```
    查看datadir值
    datadir=/var/lib/mysql
    删除/var/lib/mysql目录下的值
    [hadoop@hadoop01 mysql]$ cd /var/lib/mysql
    [hadoop@hadoop01 mysql]$ sudo rm -rf ./*
    ```
5. 初始化数据库  
    ```
    [hadoop@hadoop01 mysql]$ sudo mysqld --initialize --user=mysql
    ```
6. 查看临时生成的root用户的密码  
    ```
    [hadoop@hadoop01 mysql]$ sudo cat /var/log/mysqld.log
    2021-01-26T08:46:28.323690Z 0 [System] [MY-013169] [Server] /usr/sbin/mysqld (mysqld 8.0.23) initializing of server in progress as process 50815
    2021-01-26T08:46:28.334651Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
    2021-01-26T08:46:29.175888Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
    2021-01-26T08:46:30.085055Z 6 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: X_fRCrK9hshp
    ```
7. 启动MySQL服务  
    ```
    [hadoop@hadoop01 mysql]$ sudo systemctl start mysqld.service
    ```
8. 登录MySQL数据库  
    ```
    [hadoop@hadoop01 mysql]$ mysql -uroot -p
    Enter password:输入生成的临时密码
    ```
9. 必须先修改root用户的密码,否则执行其他的操作会报错  
    ```
    mysql> set password="root";
    ```
10. 修改mysql库下的user表中的root用户允许任意ip连接  
    ```
    mysql> update mysql.user set host='%' where user='root';
    mysql> flush privileges;
    ```

## 1.4 Hive元数据配置到MySQL  
### 1.4.1 拷贝驱动  
将MySQL的JDBC驱动拷贝到Hive的lib目录下  
```
[hadoop@hadoop01 mysql-connector-java-8.0.23]$ cp mysql-connector-java-8.0.23.jar /opt/module/apache-hive-3.1.2-bin/lib/
```

### 1.4.2 配置Metastore到MySQL  
1. 在$HIVE_HOME/conf目录下新建hive-site.xml文件  

    ```
    [hadoop@hadoop01 conf]$ vim $HIVE_HOME/conf/hive-site.xml
    添加如下内容:
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
    	<!-- jdbc 连接的 URL -->
    	<property>
    		<name>javax.jdo.option.ConnectionURL</name>
    		<value>jdbc:mysql://hadoop01:3306/metastore?useSSL=false</value>
    	</property>

    	<!-- jdbc 连接的 Driver-->
    	<property>
    		<name>javax.jdo.option.ConnectionDriverName</name>
    		<value>com.mysql.jdbc.Driver</value>
    	</property>

    	<!-- jdbc 连接的 username-->
    	<property>
    		<name>javax.jdo.option.ConnectionUserName</name>
    		<value>root</value>
    	</property>

    	<!-- jdbc 连接的 password -->
    	<property>
    		<name>javax.jdo.option.ConnectionPassword</name>
    		<value>root</value>
    	</property>

    	<!-- Hive 元数据存储版本的验证 -->
    	<property>
    		<name>hive.metastore.schema.verification</name>
    		<value>false</value>
    	</property>

    	<!--元数据存储授权-->
    	<property>
    		<name>hive.metastore.event.db.notification.api.auth</name>
    		<value>false</value>
    	</property>

    	<!-- Hive 默认在 HDFS 的工作目录 -->
    	<property>
    		<name>hive.metastore.warehouse.dir</name>
    		<value>/user/hive/warehouse</value>
    	</property>
    </configuration>
    ```
2. 登陆MySQL  
    ```
    [hadoop@hadoop01 conf]$ mysql -uroot -proot
    ```
3. 新建Hive元数据库  
    ```
    mysql> create database metastore;
    ```
4. 初始化Hive元数据库  
    ```
    [hadoop@hadoop01 conf]$ schematool -initSchema -dbType mysql -verbose
    ```

### 1.4.3 再次启动hive
1. 启动Hive  
    ```
    [hadoop@hadoop01 ~]$ hive
    ```
2. 使用Hive  
    ```
    hive> show databases;
    hive> use default;
    hive> show tables;
    hive> create table test (id int);
    hive> insert into test values(1);
    hive> select * from test;
    ```

## 1.5 使用元数据服务的方式访问Hive
1. 在hive-site.xml文件中添加如下配置信息  
    ```
    <!-- 指定存储元数据要连接的地址 -->
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://hadoop01:9083</value>
    </property>
    ```
2. 启动metastore  
    ```
    [hadoop@hadoop01 conf]$ hive --service metastore
    ```
3. 启动hive  
    ```
    [hadoop@hadoop01 conf]$ hive
    ```

## 1.6 使用JDBC方式访问Hive
1. 在hive-site.xml文件中添加如下配置信息
    ```
    <!-- 指定 hiveserver2 连接的 host -->
    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>hadoop01</value>
    </property>
    <!-- 指定 hiveserver2 连接的端口号 -->
    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
    </property>
    ```
2. 启动hiveserver2
    ```
    [hadoop@hadoop01 bin]$ hive --service hiveserver2
    ```
3. 启动beeline客户端
    ```
    [hadoop@hadoop01 bin]$ beeline -u jdbc:hive2://hadoop01:10000 -n hadoop
    ```
4. 看到如下界面
    ```
    Connecting to jdbc:hive2://hadoop01:10000
    Connected to: Apache Hive (version 3.1.2)
    Driver: Hive JDBC (version 3.1.2)
    Transaction isolation: TRANSACTION_REPEATABLE_READ
    Beeline version 3.1.2 by Apache Hive
    0: jdbc:hive2://hadoop01:10000>
    ```
5. 编写hive服务启动脚本  
   - 前台启动的方式导致需要打开多个shell窗口，可以使用如下方式后台方式启动
     - nohup:放在命令开头，表示不挂起,也就是关闭终端进程也继续保持运行状态
     - /dev/null:是Linux文件系统中的一个文件，被称为黑洞，所有写入改文件的内容 都会被自动丢弃
     - 2>&1:表示将错误重定向到标准输出上
     - &:放在命令结尾,表示后台运行
     - 一般会组合使用:`nohup [xxx 命令操作] > file	2>&1 &`，表示将xxx命令运行的结果输出到file中，并保持命令启动的进程在后台运行
   - 为了方便使用，可以直接编写脚本来管理服务的启动和关闭
        ```
        #!/bin/bash
        HIVE_LOG_DIR=$HIVE_HOME/logs
        if [ ! -d $HIVE_LOG_DIR ] then
        	mkdir -p $HIVE_LOG_DIR
        fi
        #检查进程是否运行正常，参数 1 为进程名，参数 2 为进程端口
        function check_process()
        {
        	pid=$(ps -ef 2>/dev/null | grep -v grep | grep -i $1 | awk '{print$2}')
        	ppid=$(netstat -nltp 2>/dev/null | grep $2 | awk '{print $7}' | cut - d '/' -f 1)
        	echo $pid
        	[[ "$pid" =~ "$ppid" ]] && [ "$ppid" ] && return 0 || return 1
        }

        function hive_start()
        {
        	metapid=$(check_process HiveMetastore 9083)
        	cmd="nohup hive --service metastore >$HIVE_LOG_DIR/metastore.log 2>&1 &"
        	[ -z "$metapid" ] && eval $cmd || echo "Metastroe 服务已启动" server2pid=$(check_process HiveServer2 10000)
        	cmd="nohup hiveserver2 >$HIVE_LOG_DIR/hiveServer2.log 2>&1 &"
        	[ -z "$server2pid" ] && eval $cmd || echo "HiveServer2 服务已启动"
        }

        function hive_stop()
        {
        	metapid=$(check_process HiveMetastore 9083)
        	[ "$metapid" ] && kill $metapid || echo "Metastore 服务未启动" server2pid=$(check_process HiveServer2 10000)
        	[ "$server2pid" ] && kill $server2pid || echo "HiveServer2 服务未启动"
        }

        case $1 in
        "start")
        	hive_start
        	;;
        "stop")
        	hive_stop
        	;;
        "restart")
        	hive_stop
        	sleep 2
        	hive_start
        	;;
        "status")
        	check_process HiveMetastore 9083 >/dev/null && echo "Metastore 服务运行 正常" || echo "Metastore 服务运行异常"
        	check_process HiveServer2 10000 >/dev/null && echo "HiveServer2 服务运 行正常" || echo "HiveServer2 服务运行异常"
        	;;
        *)
        	echo Invalid Args!
        	echo 'Usage: '$(basename $0)' start|stop|restart|status'
        	;;
        esac
        ```
   - 添加执行权限
    ```
    [hadoop@hadoop01 bin]$ chmod +x $HIVE_HOME/bin/hiveservices.sh
    ```
   - 启动Hive后台服务
    ```
    [hadoop@hadoop01 bin]$ hiveservices.sh start
    ```

## 1.7 使用JDBC方式访问Hive  
```
[hadoop@hadoop01 bin]$ hive --help
Usage ./hive <parameters> --service serviceName <service parameters>
Service List: beeline cleardanglingscratchdir cli fixacidkeyindex help hiveburninclient hiveserver2 hplsql jar lineage llapdump llap llapstatus metastore metatool orcfiledump rcfilecat schemaTool strictmanagedmigration tokentool version
Parameters parsed:
  --auxpath : Auxiliary jars
  --config : Hive configuration directory
  --service : Starts specific service/component. cli is default
Parameters used:
  HADOOP_HOME or HADOOP_PREFIX : Hadoop install directory
  HIVE_OPT : Hive options
For help on a particular service:
  ./hive --service serviceName --help
Debug help:  ./hive --debug --help
```  
1. “-e”不进入hive的交互窗口执行sql语句
    ```
    [hadoop@hadoop01 bin]$ hive -e "select * from test;"
    ```
2. “-f”执行脚本中sql语句
   - 在/opt/module/apache-hive-3.1.2-bin/下创建datas目录并在datas目录下创建hivef.sql文件
    ```
    [hadoop@hadoop01 apache-hive-3.1.2-bin]$ mkdir datas
    [hadoop@hadoop01 apache-hive-3.1.2-bin]$ cd datas/
    [hadoop@hadoop01 datas]$ touch hivef.sql
    ```
   - 文件中写入正确的sql语句
    ```
    select * from test;
    ```
   - 执行文件中的sql语句
    ```
    [hadoop@hadoop01 datas]$ hive -f /opt/module/apache-hive-3.1.2-bin/datas/hivef.sql
    ```
   - 执行文件中的sql语句并将结果写入文件中
    ```
    [hadoop@hadoop01 datas]$ hive -f /opt/module/apache-hive-3.1.2-bin/datas/hivef.sql > /opt/module/apache-hive-3.1.2-bin/datas/hive_result.txt
    ```

## 1.8 Hive其他命令操作
1. 退出hive窗口
    ```
    hive> exit;
    hive> quit;
    ```
2. 在hive cli命令窗口中如何查看hdfs文件系统
    ```
    hive> dfs -ls /;
    ```
3. 查看在hive中输入的所有历史命令
    ```
    进入到当前用户的根目录/root或/home/hadoop
    cd
    查看.hivehistory文件
    [hadoop@hadoop01 ~]$ cat .hivehistory
    ```

## 1.9 Hive常见属性配置
### 1.9.1 Hive运行日志信息配置
1. Hive的log默认存放在/tmp/(当前用户名下)/hive.log目录下
2. 修改hive的log存放日志到/opt/module/apache-hive-3.1.2-bin/logs
   - 修改/opt/module/hive/conf/hive-log4j2.properties.template文件名称为hive-log4j2.properties
    ```
    [hadoop@hadoop01 conf]$ mv hive-log4j2.properties.template hive-log4j2.properties
    ```
   - 在hive-log4j2.properties文件中修改log存放位置
    ```
    hive.log.dir=/opt/module/apache-hive-3.1.2-bin/logs
    ```

### 1.9.2 在hive-site.xml中加入如下两个配置:
    ```
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
    ```

### 1.9.3 参数配置方式
1. 查看当前所有的配置信息
    ```
    hive (default)> set;
    ```
2. 参数的配置三种方式
   - 配置文件方式默认配置文件：hive-default.xml;用户自定义配置文件：hive-site.xml  
   注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效  
   - 命令行参数方式  
   启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。注意：仅对本次hive启动有效  
   - 参数声明方式  
   可以在HQL中使用SET关键字设定参数。注意：仅对本次hive启动有效。  
上述三种设定方式的优先级依次递增。即配置文件<命令行参数<参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。  
