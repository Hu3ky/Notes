# 7.函数
## 7.1 系统内置函数
1. 查看系统自带的函数
    ```
    hive (default)> show functions;
    ```
2. 显示自带的函数的用法
    ```
    hive (default)> desc function upper;
    ```
3. 详细显示自带的函数的用法
    ```
    hive (default)> desc function extended upper;
    ```

## 7.2 常用内置函数
### 7.2.1 空字段赋值
1. 函数说明  
NVL：给值为NULL的数据赋值，它的格式是NVL(value，default_value)。它的功能是如果value为NULL，则NVL函数返回default_value的值，否则返回value的值，如果两个参数都为NULL，则返回NULL。  
2. 数据准备  
采用员工表  
3. 查询：如果员工的comm为NULL，则用-1代替  
```
hive (default)> select comm,nvl(comm,-1) from emp;
```
4. 查询：如果员工的comm为NULL，则用领导id代替  
```
hive (default)> select comm,nvl(comm,mgr) from emp;
```

### 7.2.2 CASE WHEN THEN ELSE END
1. 数据准备  

| name | dept_id | sex |
| :--: | :--: | :--: |
| 悟空 | A | 男 |
| 大海 | A | 男 |
| 宋宋 | B | 男 |
| 凤姐 | A | 女 |
| 婷姐 | B | 女 |
| 婷婷 | B | 女 |  

2. 需求  
求出不同部门男女各多少人。结果如下:  

| dept_Id | 男 | 女 |
| :--: | :--: | :--: |
| A | 2 | 1 |
| B | 1 | 2 |

3. 创建本地emp_sex.txt，导入数据  
```
悟空	A	男
大海	A	男
宋宋	B	男
凤姐	A	女
婷姐	B	女
婷婷	B	女
```
4. 创建hive表并导入数据  
```
hive (default)> create table emp_sex(
              > name string,
              > dept_id string,
              > sex string)
              > row format delimited fields terminated by "\t";
OK
Time taken: 1.041 seconds
hive (default)> load data local inpath '/opt/module/apache-hive-3.1.2-bin/datas/emp_sex.txt' into table emp_sex;
```
5. 按需求查询数据
```
hive (default)> select
              > dept_id,
              > sum(case sex when '男' then 1 else 0 end) male_count,
              > sum(case sex when '女' then 1 else 0 end) female_count
              > from emp_sex
              > group by dept_id;
```

### 7.2.3 行转列
1. 相关函数说明  
`CONCAT(string A/col, string B/col…)`：返回输入字符串连接后的结果，支持任意个输入字符串;  
`CONCAT_WS(separator, str1, str2,...)`：它是一个特殊形式的CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是NULL，返回值也将为NULL。这个函数会跳过分隔符参数后的任何NULL和空字符串。分隔符将被加到被连接的字符串之间;  
注意:CONCAT_WS must be string or array<string>  
`COLLECT_SET(col)`：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生Array类型字段  
2. 数据准备  

| name | constellation | blood_type |
| :--: | :--: | :--: |
| 孙悟空 | 白羊座 | A |
| 大海 | 射手座 | A |
| 宋宋 | 白羊座 | B |
| 猪八戒 | 白羊座 | A |
| 凤姐 | 射手座 | A |
| 苍老师 | 白羊座 | B |

3. 需求  
把星座和血型一样的人归类到一起。结果如下  
```
射手座,A	大海|凤姐
白羊座,A	孙悟空|猪八戒
白羊座,B	宋宋|苍老师
```
4. 创建本地person_info.txt，导入数据
```
孙悟空	白羊座	A
大海	射手座	A
宋宋	白羊座	B
猪八戒	白羊座	A
凤姐	射手座	A
苍老师	白羊座	B
```
5. 创建hive表并导入数据
```
hive (default)> create table person_info( name string, constellation string, blood_type string)
              > row format delimited fields terminated by "\t";
hive (default)> load data local inpath "/opt/module/apache-hive-3.1.2-bin/datas/person_info.txt" into table person_info;
```
6. 按需求查询数据
```
hive (default)> select
              >     t1.c_b,
              >     CONCAT_WS('|',COLLECT_SET(t1.name))
              > from
              >     (select
              >         name,
              >         CONCAT_WS(',',constellation,blood_type) c_b
              >     from
              >         person_info
              >     )t1
              > group by t1.c_b;
```

### 7.2.4 列转行
1. 函数说明  
EXPLODE(col)：将hive一列中复杂的Array或者Map结构拆分成多行  
LATERAL VIEW:  
用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias  
解释：用于和split,explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合  
2. 数据准备  

| movie | category |
| :--: | :--: |
| 《疑犯追踪》 | 悬疑,动作,科幻,剧情 |
| 《Lie to me》 | 悬疑,警匪,动作,心理,剧情 |
| 《战狼 2》 | 战争,动作,灾难 |

3. 需求  
将电影分类中的数组数据展开。结果如下  
```
《疑犯追踪》	悬疑
《疑犯追踪》	动作
《疑犯追踪》	科幻
《疑犯追踪》	剧情
《Lie to me》	悬疑
《Lie to me》	警匪
《Lie to me》	动作
《Lie to me》	心理
《Lie to me》	剧情
《战狼 2》	战争
《战狼 2》	动作
《战狼 2》	灾难
```
4. 创建本地movie_info.txt，导入数据
```
《疑犯追踪》	悬疑,动作,科幻,剧情
《Lie to me》	悬疑,警匪,动作,心理,剧情
《战狼 2》	战争,动作,灾难
```
5. 创建hive表并导入数据
```
hive (default)> create table movie_info( movie string, category string)
              > row format delimited fields terminated by "\t";
hive (default)> load data local inpath "/opt/module/apache-hive-3.1.2-bin/datas/movie_info.txt" into table movie_info;
```
6. 按需求查询数据
```
hive (default)> SELECT
              >     movie,
              >     category_name
              > FROM
              >     movie_info
              > lateral VIEW
              >     explode(split(category,",")) movie_info_tmp AS category_name;
```

### 7.2.5 窗口函数(开窗函数)  
1. 相关函数说明:  
   - `OVER()`:指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化  
   - `CURRENT ROW`:当前行
     - `n PRECEDING`:往前n行数据
     - `n FOLLOWING`:往后n行数据
   - `UNBOUNDED`:起点
     - `UNBOUNDED PRECEDING`:表示从前面的起点
     - `UNBOUNDED FOLLOWING`:表示到后面的终点
   - `LAG(col,n,default_val)`:往前第n行数据
   - `LEAD(col,n, default_val)`:往后第n行数据
   - `NTILE(n)`:把有序窗口的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。
2. 数据准备:
```
name,orderdate,cost
jack,2017-01-01,10
tony,2017-01-02,15
jack,2017-02-03,23
tony,2017-01-04,29
jack,2017-01-05,46
jack,2017-04-06,42
tony,2017-01-07,50
jack,2017-01-08,55
mart,2017-04-08,62
mart,2017-04-09,68
neil,2017-05-10,12
mart,2017-04-11,75
neil,2017-06-12,80
mart,2017-04-13,94
```
3. 需求
   - 查询在2017年4月份购买过的顾客及总人数
   - 查询顾客的购买明细及月购买总额
   - 上述的场景, 将每个顾客的cost按照日期进行累加
   - 查询每个顾客上次的购买时间
   - 查询前20%时间的订单信息
4. 创建本地business.txt，导入数据
```
[hadoop@hadoop01 datas]$ vi business.txt
```
5. 创建hive表并导入数据
```
hive (default)> create table business(
              > name string,
              > orderdate string,
              > cost int
              > ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
```
```
hive (default)> load data local inpath '/opt/module/apache-hive-3.1.2-bin/datas/business.txt' into table business;
```
6. 按需求查询数据
   - 查询在2017年4月份购买过的顾客及总人数
    ```
    hive (default)> select name,count(*) over()
                  > from business
                  > where substring(orderdate,1,7)='2017-04'
                  > group by name;
    ```
   - 查询顾客的购买明细及月购买总额
    ```
    hive (default)> select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from business;
    ```
   - 将每个顾客的 cost 按照日期进行累加
    ```
    hive (default)> select name,orderdate,cost,
                  > sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row)
                  > from business;
    ```
   - 查看顾客上次的购买时间
    ```
    hive (default)> select name,orderdate,cost,
                  > lag(orderdate,1,'1900-01-01') over(partition by name order by orderdate ) as time1
                  > from business;
    ```
   - 查询前20%时间的􏰁单信息
    ```
    hive (default)> select * from
                  >     (
                  >     select name,orderdate,cost, ntile(5) over(order by orderdate) sorted from business
                  >     )t
                  > where sorted = 1;
    ```

### 7.2.6 Rank
1. 函数说明
   - `RANK()`:排序相同时会重复，总数不会变
   - `DENSE_RANK()`:排序相同时会重复，总数会减少
   - `ROW_NUMBER()`:会根据顺序计算
2. 数据准备

| name | subject | score |
| :--: | :--: | :--: |
| 孙悟空 | 语文 | 87 |
| 孙悟空 | 数学 | 95 |
| 孙悟空 | 英语 | 68 |
| 大海 | 语文 | 94 |
| 大海 | 数学 | 56 |
| 大海 | 英语 | 84 |
| 宋宋 | 语文 | 64 |
| 宋宋 | 数学 | 86 |
| 宋宋 | 英语 | 84 |
| 婷婷 | 语文 | 65 |
| 婷婷 | 数学 | 85 |
| 婷婷 | 英语 | 78 |

3. 需求  
计算每门学科成绩排名  
4. 创建本地score.txt，导入数据
```
[hadoop@hadoop01 datas]$ vi score.txt
```
5. 创建hive表并导入数据
```
hive (default)> create table score(
              > name string,
              > subject string,
              > score int)
              > row format delimited fields terminated by "\t";
```
```
hive (default)> load data local inpath '/opt/module/apache-hive-3.1.2-bin/datas/score.txt' into table score;
```
6. 按需求查询数据
```
hive (default)> select name,
              > subject,
              > score,
              > rank() over(partition by subject order by score desc) rp,
              > dense_rank() over(partition by subject order by score desc) drp,
              > row_number() over(partition by subject order by score desc) rmp
              > from score;
```

### 7.2.7 其他常用函数

## 7.3 自定义函数
1. Hive自带了一些函数，比如:max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。  
2. 当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数(UDF:user-defined function)。
3. 根据用户自定义函数类别分为以下三种:
   - UDF(User-Defined-Function):一进一出
   - UDAF(User-Defined Aggregation Function):聚集函数，多进一出 类似于:count/max/min
   - UDTF(User-Defined Table-Generating Functions):一进多出,如lateral view explode()
4. 官方文档地址:https://cwiki.apache.org/confluence/display/Hive/HivePlugins
5. 编程步骤:
   - 继承Hive提供的类
    ```
    org.apache.hadoop.hive.ql.udf.generic.GenericUDF
    org.apache.hadoop.hive.ql.udf.generic.GenericUDTF
    ```
   - 实现类中的抽象方法
   - 在hive的命令行窗口创建函数添加jar,创建function
    ```
    add jar linux_jar_path
    create [temporary] function [dbname.]function_name AS class_name;
    ```
   - 在hive的命令行窗口删除函数
    ```
    drop [temporary] function [if exists] [dbname.]function_name;
    ```

## 7.4 自定义UDF函数
0. 需求:  
自定义一个UDF实现􏰀算给定字符串的长度，例如:
```
hive(default)> select my_len("abcd");
4
```
1. 创建一个Maven工程Hive
2. 导入依赖
```
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-exec</artifactId>
    <version>3.1.2</version>
</dependency>
```
3. 创建一个类
```
package com.hu3ky.udf;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

public class MyUDF extends GenericUDF {
    @Override
    public ObjectInspector initialize(ObjectInspector[] objectInspectors) throws UDFArgumentException {
//        判断输入参数的个数
        if (objectInspectors.length != 1) {
            throw new UDFArgumentLengthException("Input Args Length Error!");
        }

        return PrimitiveObjectInspectorFactory.javaIntObjectInspector;
    }

    @Override
    public Object evaluate(DeferredObject[] deferredObjects) throws HiveException {
//        1.输入数据
        String input = deferredObjects[0].get().toString();
//        2.判断输入数据是否为空
        if (input == null) {
            return 0;
        }
//        3. 返回输入数据的长度
        return input.length();
    }

    @Override
    public String getDisplayString(String[] strings) {
        return "";
    }
}

```
4. 打成jar包上传到服务器
```
/opt/module/apache-hive-3.1.2-bin/lib/hive-demo-1.0-SNAPSHOT.jar
```
5. 将jar包添加到hive的classpath
```
hive (default)> add jar /opt/module/apache-hive-3.1.2-bin/lib/hive-demo-1.0-SNAPSHOT.jar;
```
6. 创建临时函数与开发好的java class关联
```
hive (default)> create temporary function my_len as "com.hu3ky.udf.MyUDF";
```
7. 即可在hql中使用自定义的函数
```
hive (default)> select my_len("abcd");
```

## 7.5 自定义UDTF函数
0. 需求  
自定义一个UDTF实现将一个任意分割符的字符串切割成独立的单词，例如:
```
hive(default)> select myudtf("hello,world,hadoop,hive", ",");
hello
world
hadoop
hive
```
1. 代码实现
```
package com.hu3ky.udtf;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

import java.util.ArrayList;
import java.util.List;

public class MyUDTF extends GenericUDTF {

    private ArrayList<String> outList = new ArrayList<>();

    @Override
    public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException {
        List<String> fieldNames = new ArrayList<>();
        List<ObjectInspector> fieldOIs = new ArrayList<>();
        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
    }

    @Override
    public void process(Object[] objects) throws HiveException {
        String args = objects[0].toString();
        String splitKey = objects[1].toString();
        String[] fields = args.split(splitKey);
        for (String field : fields) {
            outList.clear();
            outList.add(field);
            forward(outList);
        }
    }

    @Override
    public void close() throws HiveException {

    }
}
```
2. 打成jar包上传到服务器
```
/opt/module/apache-hive-3.1.2-bin/lib/hive-demo-1.0-SNAPSHOT.jar
```
3. 将jar包添加到hive的classpath下
```
hive (default)> add jar /opt/module/apache-hive-3.1.2-bin/lib/hive-demo-1.0-SNAPSHOT.jar;
```
4. 创建临时函数与开发好的java class关联
```
hive (default)> create temporary function myudtf as "com.hu3ky.udtf.MyUDTF";
```
5. 使用自定义的函数
```
hive (default)> select myudtf("hello,world,hadoop,hive",",");
```
