# 7.HDFS客户端操作
## 7.1 HDFS客户端准备
1. 根据自己电脑的操作系统拷贝对应的编译后的hadoop.jar包到非中文路径。
2. 配置HADOOP_HOME环境变量。
3. 配置PATH环境变量
4. 创建一个Maven工程HdfsClientDemo
5. 导入相应的依赖坐标+日志添加
    ```
    <dependencies>
            <!-- https://mvnrepository.com/artifact/junit/junit -->
            <dependency>
                <groupId>junit</groupId>
                <artifactId>junit</artifactId>
                <version>RELEASE</version>
            </dependency>
            <!-- https://mvnrepository.com/artifact/org.apache.logging.log4j/log4j-core -->
            <dependency>
                <groupId>org.apache.logging.log4j</groupId>
                <artifactId>log4j-core</artifactId>
                <version>2.12.1</version>
            </dependency>
            <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common -->
            <dependency>
                <groupId>org.apache.hadoop</groupId>
                <artifactId>hadoop-common</artifactId>
                <version>3.3.0</version>
            </dependency>
            <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client -->
            <dependency>
                <groupId>org.apache.hadoop</groupId>
                <artifactId>hadoop-client</artifactId>
                <version>3.3.0</version>
            </dependency>
            <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs -->
            <dependency>
                <groupId>org.apache.hadoop</groupId>
                <artifactId>hadoop-hdfs</artifactId>
                <version>3.3.0</version>
            </dependency>

    </dependencies>
    ```
    ```
    log4j.rootLogger=INFO, stdout
    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
    log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
    log4j.appender.logfile=org.apache.log4j.FileAppender
    log4j.appender.logfile.File=target/spring.log
    log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
    log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
    ```

## 7.2 HDFS的API操作
### 7.2.1 HDFS文件上传(测试参数优先级)
1. 编写源码
    ```
    /**
     * 1. 文件上传
     */
    @Test
    public void testCopyFromLocalFile() throws URISyntaxException, IOException, InterruptedException {
        //1. 获取fs对象
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://hadoop01:9000"), conf, "hadoop");
        //2. 执行上传代码
        fs.copyFromLocalFile(new Path("/test.txt"), new Path("/test.txt"));
        //3. 关闭资源
        fs.close();
    }
    ```
2. 将hdfs-site.xml拷贝到项目的根目录下
    ```
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
        <!-- 副本数 -->
        <property>
            <name>dfs.replication</name>
            <value>1</value>
        </property>
    </configuration>
    ```
3. 参数优先级  
参数优先级排序:(1)客户端代码中设置的值>(2)ClassPath下的用户自定义配置文件>(3)服务器的默认配置

### 7.2.2 HDFS文件下载
```
/**
 * 2. 文件下载
 */
@Test
public void testCopyToLocalFile() throws URISyntaxException, IOException, InterruptedException {
    //1. 获取fs对象

    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.get(new URI("hdfs://hadoop01:9000"), conf, "hadoop");
    //2. 执行下载操作
    fs.copyToLocalFile(new Path("/repTest.txt"), new Path("/repTest.txt"));
    //3. 关闭资源
    fs.close();
}
```

### 7.2.3 HDFS文件夹删除
```
@Test
public void testDelete() throws URISyntaxException, IOException, InterruptedException {
    //1. 获取对象
    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.get(new URI("hdfs://hadoop01:9000"), conf, "hadoop");
    //2. 文件删除
    fs.delete(new Path("/0529"),true);
    //3. 关闭资源
    fs.close();
}
```

### 7.2.4 HDFS文件名更改
```
/**
 * 4. 文件更名
 */
@Test
public void testRename() throws URISyntaxException, IOException, InterruptedException {
    //1. 获取对象
    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.get(new URI("hdfs://hadoop01:9000"), conf, "hadoop");
    //2. 执行更名操作
    fs.rename(new Path("/test.txt"), new Path("/testRename.txt"));
    //3. 关闭资源
    fs.close();
}
```

### 7.2.5 HDFS文件详情查看
```
/**
 * 5. 文件详情查看
 */
@Test
public void testListFiles() throws URISyntaxException, IOException, InterruptedException {
    //1. 获取对象
    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.get(new URI("hdfs://hadoop01:9000"), conf, "hadoop");
    //2. 查看文件详情
    RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path("/"), true);

    while (listFiles.hasNext()) {
        // 查看文件名称、权限、长度、块信息
        LocatedFileStatus fileStatus = listFiles.next();
        //文件名称
        System.out.println(fileStatus.getPath().getName());
        //文件权限
        System.out.println(fileStatus.getPermission());
        //文件长度
        System.out.println(fileStatus.getLen());

        BlockLocation[] blockLocations = fileStatus.getBlockLocations();
        for (BlockLocation blockLocation : blockLocations) {
            String[] hosts = blockLocation.getHosts();
            for (String host: hosts) {
                System.out.println(host);
            }
        }
        System.out.println("------分割线-------");
    }
    //3. 关闭资源
    fs.close();
}
```

### 7.2.6 HDFS文件和文件夹判断
```
/**
 * 6. 判断是文件还是文件夹
 */
@Test
public void testListStatus() throws URISyntaxException, IOException, InterruptedException {
    //1. 获取对象
    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.get(new URI("hdfs://hadoop01:9000"), conf, "hadoop");
    //2. 判断操作
    FileStatus[] listStatus = fs.listStatus(new Path("/"));
    for (FileStatus fileStatus: listStatus) {
        if (fileStatus.isFile()) {
            //文件
            System.out.println("f:" + fileStatus.getPath().getName());
        } else {
            //文件夹
            System.out.println("d:" + fileStatus.getPath().getName());
        }
    }
    //3. 关闭资源
    fs.close();
}
```

## 7.3 HDFS的I/O操作
上面的API操作HDFS系统都是框架封装好的。那么如果想要时下上述API的操作该怎么实现呢？  
可以采用IO流的方式实现数据的上传和下载。

### 7.3.1 HDFS文件上传
1. 需求:把本地根目录下的testUpload.txt文件上传到HDFS根目录
2. 编写代码:
```
@Test
public void putFileToHDFS() throws URISyntaxException, IOException, InterruptedException {
    //1. 获取对象
    Configuration conf = new Configuration();
    FileSystem fileSystem = FileSystem.get(new URI("hdfs://hadoop01:9000"), conf, "hadoop");
    //2. 获取输入流
    FileInputStream inputStream = new FileInputStream(new File("/testUpload.txt"));
    //3. 获取输入流
    FSDataOutputStream outputStream = fileSystem.create(new Path("/testUpload.txt"));
    //4. 流的对拷
    IOUtils.copyBytes(inputStream, outputStream, conf);
    //5. 关闭资源
    IOUtils.closeStream(outputStream);
    IOUtils.closeStream(inputStream);
    fileSystem.close();
}
```

### 7.3.2 HDFS文件下载
1. 需求:从HDFS上下载repTest.txt到本地根目录下
2. 编写代码:
```
@Test
public void getFileFromHDFS() throws URISyntaxException, IOException, InterruptedException {
    //1. 获取对象
    Configuration conf = new Configuration();
    FileSystem fileSystem = FileSystem.get(new URI("hdfs://hadoop01:9000"), conf, "hadoop");
    //2. 获取输入流
    FSDataInputStream inputStream = fileSystem.open(new Path("/repTest.txt"));
    //3. 获取输出流
    FileOutputStream outputStream = new FileOutputStream(new File("/repTest.txt"));
    //4. 流的对拷
    IOUtils.copyBytes(inputStream, outputStream, conf);
    //5. 关闭资源
    IOUtils.closeStream(outputStream);
    IOUtils.closeStream(inputStream);
    fileSystem.close();
}
```

### 7.3.3 文件定位读取
1. 需求:分块读取HDFS上的大文件，比如根目录下的hadoop-3.3.0.tar.gz
2. 编写代码:
```
//下载第一块
@Test
public void readFileSeek1() throws URISyntaxException, IOException, InterruptedException {
    //1. 获取对象
    Configuration conf = new Configuration();
    FileSystem fileSystem = FileSystem.get(new URI("hdfs://hadoop01:9000"), conf, "hadoop");
    //2. 获取输入流
    FSDataInputStream inputStream = fileSystem.open(new Path("/hadoop-3.3.0.tar.gz"));
    //3. 获取输出流
    FileOutputStream outputStream = new FileOutputStream(new File("/hadoop-3.3.0.tar.gz.part1"));
    //4. 流的对拷(只拷128M)
    byte[] buf = new byte[1024];
    for (int i = 0; i < 1024 * 128; i++) {
        inputStream.read(buf);
        outputStream.write(buf);
    }
    //5. 关闭资源
    IOUtils.closeStream(outputStream);
    IOUtils.closeStream(inputStream);
    fileSystem.close();
}
//从第二块开始，下载所有的数据块
@Test
public void readFileSeek2() throws URISyntaxException, IOException, InterruptedException {
    //1. 获取对象
    Configuration conf = new Configuration();
    FileSystem fileSystem = FileSystem.get(new URI("hdfs://hadoop01:9000"), conf, "hadoop");
    //2. 获取输入流
    FSDataInputStream inputStream = fileSystem.open(new Path("/hadoop-3.3.0.tar.gz"));
    //3. 设置指定读取的起点
    inputStream.seek(1024 * 1024 * 128);
    //4. 获取输出流
    FileOutputStream outputStream = new FileOutputStream(new File("/hadoop-3.3.0.tar.gz.part2"));
    //5. 流的对拷
    IOUtils.copyBytes(inputStream, outputStream, conf);
    //6. 关闭资源
    IOUtils.closeStream(outputStream);
    IOUtils.closeStream(inputStream);
    fileSystem.close();
}
```
