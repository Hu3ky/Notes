# 12.MapReduce概述
## 12.1 MapReduce定义
1. MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。
2. MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并运行在一个Hadoop集群上。

## 12.2 MapReduce优缺点
### 12.2.1 优点
1. MapReduce易于编程  
它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。  
2. 良好的扩展性  
当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。  
3. 高容错性  
MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。  
4. 适合PB级以上海量数据的离线处理  
可以实现上千台服务器集群并发工作，提供数据处理能力。  

### 12.2.2 缺点
1. 不擅长实时计算  
MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果。  
2. 不擅长流式计算  
流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。  
3. 不擅长DAG(有向图)计算  
多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。  

## 12.3 MapReduce核心思想
1. 分布式的运算程序往往需要分成至少2个阶段。
2. 第一个阶段的MapTask并发实例，完全并行运行，互不相干。
3. 第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。
4. MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。

## 12.4 MapReduce进程
一个完整的MapReduce程序在分布式运行时有三类实例进程:  
1. MrAppMaster:负责整个程序过程的调度及状态协调。
2. MapTask:负责Map阶段的整个数据处理流程。
3. ReduceTask:负责Reduce阶段的整个数据处理流程。

## 12.5 官方WordCount源码

## 12.6 常用数据序列化类型
常用的数据类型对应的Hadoop数据序列化类型  

| Java类型 | Hadoop Writable类型 |
| :-- | :-- |
| boolean | BooleanWritable |
| byte | ByteWritable |
| int | IntWritable |
| float | FloatWritable |
| long | LongWritable |
| double | DoubleWritable |
| String | Text |
| map | MapWritable |
| array | ArrayWritable |

## 12.7 MapReduce编程规范
用户编写的程序分成三个部分:Mapper、Reducer和Driver。  
1. Mapper阶段
   - 用户自定义的Mapper要继承自己的父类
   - Mapper的输入数据是KV对的形式(KV的类型可自定义)
   - Mapper中的业务逻辑写在map()方法中
   - Mapper的输出数据是KV对的形式(KV的类型可自定义)
   - map()方法(MapTask进程)对每一个<K,V>调用一次
2. Reducer阶段
   - 用户自定义的Reducer要继承自己的父类
   - Reducer的输入数据类型对应Mapper的输出数据类型，也是KV
   - Reducer的业务逻辑写在reduce()方法中
   - ReduceTask进程对每一组相同k的<k,v>组调用一次reduce()方法
3. Driver阶段
   - 相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象

## 12.8 WordCount 案例实操
1. 需求  
在给定的文本文件中统计输出每一个单词出现的总次数  
2. 需求分析  
按照MapReduce编程规范，分别编写Mapper，Reducer，Driver。  
   - 输入数据
   - 输出数据
   - Mapper
     - 将MapTask传给我们的文本内容先转换成String
     - 根据空格将这一行切分成单词
     - 将单词输出为<单词，1>
   - Reducer
     - 汇总各个key的个数
     - 输出该key的总次数
   - Driver
     - 获取配置信息，获取job对象实例
     - 指定本程序的jar包所在的本地路径
     - 关联Mapper/Reducer业务类
     - 指定Mapper输出数据的kv类型
     - 指定最终输出的数据的kv类型
     - 指定job的输入原始文件所在目录
     - 指定job的输出结果所在目录
     - 提交作业
3. 环境准备
   - 创建maven工程
   - 在pom.xml文件中添加如下依赖
    ```
    <dependencies>
        <!-- https://mvnrepository.com/artifact/junit/junit -->
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>RELEASE</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.logging.log4j/log4j-core -->
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-core</artifactId>
            <version>2.12.1</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>3.3.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>3.3.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>3.3.0</version>
        </dependency>

    </dependencies>
    ```
   - 在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入如下内容：
    ```
    log4j.rootLogger=INFO, stdout
    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
    log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
    log4j.appender.logfile=org.apache.log4j.FileAppender
    log4j.appender.logfile.File=target/spring.log
    log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
    log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
    ```
4. 编写程序
   - 编写Mapper类
    ```
    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;
    import java.io.IOException;

    // map阶段
    // KEYIN：输入数据的key类型
    // VALUEIN：输入数据的value类型
    // KEYOUT：输出数据的key类型
    // VALUEOUT：输出数据的value类型
    public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            Text k = new Text();
            IntWritable v = new IntWritable(1);
            // 1.获取一行
            String line = value.toString();
            // 2.切割单词
            String[] words = line.split(" ");
            // 3.循环写出
            for (String word: words) {
                k.set(word);
                context.write(k, v);
            }
        }
    }
    ```
   - 编写Reducer类
    ```
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Reducer;
    import java.io.IOException;
    // KEYIN, VALUEIN map阶段输出的key和value
    public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        int sum;
        IntWritable v = new IntWritable();
        @Override
        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            // 1.累加求和
            sum = 0;
            for (IntWritable number : values) {
                sum += number.get();
            }
            // 2.输出
            v.set(sum);
            context.write(key, v);
        }
    }
    ```
   - 编写Driver驱动类
    ```
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

    import java.io.IOException;

    public class WordcountDriver {
        public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
            Configuration conf = new Configuration();
            //1. 获取Job对象
            Job job = Job.getInstance(conf);
            //2. 设置jar存储路径
            job.setJarByClass(WordcountDriver.class);
            //3. 关联Map和Reduce类
            job.setMapperClass(WordCountMapper.class);
            job.setReducerClass(WordCountReducer.class);
            //4. 设置Mapper阶段输出数据的key和value类型
            job.setMapOutputKeyClass(Text.class);
            job.setMapOutputValueClass(IntWritable.class);
            //5. 设置最终数据输出的key和value类型
            job.setOutputKeyClass(Text.class);
            job.setMapOutputValueClass(IntWritable.class);
            //6. 设置输入路径和输出路径
            FileInputFormat.setInputPaths(job, new Path(args[0]));
            FileOutputFormat.setOutputPath(job, new Path(args[1]));
            //7. 提交job
            boolean result = job.waitForCompletion(true);
            System.exit(result ? 0 : 1);
        }
    }
    ```
5. 本地测试
   - 输入文件内容:
    ```
    Hello world
    This is a test
    Hello everyone
    Are you ok
    ```
   - 输出文件内容
    ```
    Are	1
    Hello	2
    This	1
    a	1
    everyone	1
    is	1
    ok	1
    test	1
    world	1
    you	1
    ```
6. 在集群上测试
   - 用maven打jar包，需要添加的打包插件依赖
    ```
    <build>
        <plugins>
            <plugin>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.1</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                    <archive>
                        <manifest>
                            <mainClass>com.hadoop.mr.wordcount.WordcountDriver</mainClass>
                        </manifest>
                    </archive>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
    ```
- 将程序打成jar包，然后拷贝到Hadoop集群中
    ```
    点击Maven，然后点击install。
    ```
- 启动 Hadoop 集群
- 执行 WordCount 程序
    ```
    hadoop jar mr-1.0-SNAPSHOT.jar com.hadoop.mr.wordcount.WordcountDriver /user/hadoop/zaiyiqi.txt /user/hadoop/output
    ```
